{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#encoding utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from IPython.display import Image\n",
    "import json\n",
    "import urllib2\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN_POS_PAIR = pd.read_pickle('./NN_POS_PAIR')\n",
    "NV_POS_PAIR = pd.read_pickle('./NV_POS_PAIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "# 要增加 pos 認識的 term 請在這裏\n",
    "# ex: dff_soup 要多認識一個名詞 '濃稠度\n",
    "#'data = { 'NPOS': np.array(['dft_soup']), 'NTERM': np.array(['濃稠度])}  \n",
    "# NN_POS_PAIR = NN_POS_PAIR.append(pd.DataFrame(data),ignore_index=True)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize word2vector model\n",
    "model=gensim.models.Word2Vec.load_word2vec_format('../02_Word2Vector/bin/mobile01_ptt.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 資料觀察用, 儲存 word2vector 失敗的的 term\n",
    "ptt_no_data = []\n",
    "for idx in range(len(NN_POS_PAIR)):\n",
    "    NN_NPOS = NN_POS_PAIR['NPOS'][idx]\n",
    "    NN_NTERM= NN_POS_PAIR['NTERM'][idx]\n",
    "    #print NN_NPOS + '\\t' + repr(NN_NTERM).decode('unicode-escape')\n",
    "    try:\n",
    "        w2v_result = model.most_similar(NN_NTERM)\n",
    "        \n",
    "    except KeyError:\n",
    "        # ERROR HANDLEING...SAVE THOSE TERM AS KNOWLEDGE\n",
    "        print 'Oops!  [' + str(idx) +']\\t' + repr(NN_NTERM).decode('unicode-escape') + 'is not work in word2vector'\n",
    "        ptt_no_data.append(NN_NTERM)\n",
    "        continue\n",
    "        \n",
    "    # FOR DATA OBSERVATION...\n",
    "    pp = pd.DataFrame(np.array(w2v_result))\n",
    "    pos_numpy = pd.DataFrame.as_matrix(pp)\n",
    "    DATA_STATISTIC_RESHAPE = pd.DataFrame(pos_numpy.reshape(2,-1))\n",
    "    #print DATA_STATISTIC_RESHAPE  \n",
    "    \n",
    "#儲存 failed term\n",
    "# PTT_NO_DATA_LIST = pd.DataFrame(ptt_no_data)\n",
    "# PTT_NO_DATA_LIST.to_pickle('./PTT_NO_DATA_N-LIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 延伸所有的 n\n",
    "exception_case = [u'鼎泰豐',u'瘦肉精']\n",
    "ptt_no_data = []\n",
    "NN_POS_PAIR_EXT = NN_POS_PAIR\n",
    "for idx in range(len(NN_POS_PAIR)):\n",
    "    NN_NPOS = NN_POS_PAIR['NPOS'][idx]\n",
    "    NN_NTERM= NN_POS_PAIR['NTERM'][idx]\n",
    "    #有些已知 word2vector找出來效果很差的就跳過\n",
    "    if NN_NTERM in exception_case:\n",
    "        continue\n",
    "    #print NN_NPOS + '\\t' + repr(NN_NTERM).decode('unicode-escape')\n",
    "    try:\n",
    "        w2v_result = model.most_similar(NN_NTERM)\n",
    "        \n",
    "    except KeyError:\n",
    "        # 還是會有錯誤的,但是這裏就不處理了\n",
    "        #print 'Oops!  [' + str(idx) +']\\t' + repr(NN_NTERM).decode('unicode-escape') + 'is not work in word2vector'\n",
    "        #ptt_no_data.append(NN_NTERM)\n",
    "        continue\n",
    "    # Expansion data\n",
    "    w2v_np_result = np.array(w2v_result)\n",
    "    w2v_np_result = w2v_np_result[:,0] #刪除成績部分,只留下term\n",
    "    data = {'NPOS': np.array([NN_POS_PAIR['NPOS'][idx]]*10),'NTERM':w2v_np_result}\n",
    "    NN_POS_PAIR_EXT = NN_POS_PAIR_EXT.append(pd.DataFrame(data),ignore_index=True)  \n",
    "    #print DATA_STATISTIC_RESHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 延伸所有的 V\n",
    "exception_case = [u'鼎泰豐',u'瘦肉精']\n",
    "ptt_no_data = []\n",
    "NV_POS_PAIR_EXT = NV_POS_PAIR\n",
    "for idx in range(len(NV_POS_PAIR)):\n",
    "    NV_NPOS = NV_POS_PAIR['NPOS'][idx]\n",
    "    NV_VTERM= NV_POS_PAIR['VTERM'][idx]\n",
    "    #有些已知 word2vector找出來效果很差的就跳過\n",
    "    if NV_VTERM in exception_case:\n",
    "        continue\n",
    "    #print NN_NPOS + '\\t' + repr(NN_NTERM).decode('unicode-escape')\n",
    "    try:\n",
    "        w2v_result = model.most_similar(NV_VTERM)\n",
    "        \n",
    "    except KeyError:\n",
    "        # 還是會有錯誤的,但是這裏就不處理了\n",
    "        #print 'Oops!  [' + str(idx) +']\\t' + repr(NN_NTERM).decode('unicode-escape') + 'is not work in word2vector'\n",
    "        #ptt_no_data.append(NN_NTERM)\n",
    "        continue\n",
    "    # Expansion data\n",
    "    w2v_np_result = np.array(w2v_result)\n",
    "    w2v_np_result = w2v_np_result[:,0] #刪除成績部分,只留下term\n",
    "    data = {'NPOS': np.array([NV_POS_PAIR['NPOS'][idx]]*10),'VTERM':w2v_np_result}\n",
    "    NV_POS_PAIR_EXT = NV_POS_PAIR_EXT.append(pd.DataFrame(data),ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#刪除重複元素909->861,然後儲存\n",
    "print len(NN_POS_PAIR_EXT)\n",
    "NN_POS_PAIR_EXT = NN_POS_PAIR_EXT.drop_duplicates()\n",
    "print len(NN_POS_PAIR_EXT)\n",
    "NN_POS_PAIR_EXT.sort(['NPOS','NTERM'],ascending=[1,1])\n",
    "NN_POS_PAIR_EXT.to_pickle('./NN_POS_PAIR_EXT') \n",
    "\n",
    "print len(NV_POS_PAIR_EXT)\n",
    "NV_POS_PAIR_EXT = NV_POS_PAIR_EXT.drop_duplicates()\n",
    "print len(NV_POS_PAIR_EXT)\n",
    "NV_POS_PAIR_EXT.sort(['NPOS','VTERM'],ascending=[1,1])\n",
    "NV_POS_PAIR_EXT.to_pickle('./NV_POS_PAIR_EXT') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "# 以下爲測試code\n",
    "#############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             dtfsoup\n",
       "1      dtfbranchkindb\n",
       "2          ingredient\n",
       "3         dtffoodkind\n",
       "4            dtfbrand\n",
       "5       dtffoodsaftya\n",
       "6         dtffoodkind\n",
       "7        dtfappetizer\n",
       "8       dtffoodsaftyb\n",
       "9       dtfingredient\n",
       "10         ingredient\n",
       "11         ingredient\n",
       "12                  n\n",
       "13                  n\n",
       "14     dtfbranchkinda\n",
       "15         ingredient\n",
       "16         ingredient\n",
       "17      dtffoodsaftyb\n",
       "18         ingredient\n",
       "19        dtffoodtype\n",
       "20      dtfingredient\n",
       "21        dtffoodkind\n",
       "22        dtffoodtype\n",
       "23          dtfbranch\n",
       "24        dtffoodtype\n",
       "25      dtffoodsaftya\n",
       "26        dtffoodkind\n",
       "27      dtffoodsaftyb\n",
       "28                  n\n",
       "29      dtffoodsaftya\n",
       "            ...      \n",
       "79            dtfsoup\n",
       "80            dtfsoup\n",
       "81           dtfbrand\n",
       "82           dtfbrand\n",
       "83           dtfbrand\n",
       "84      dtfingredient\n",
       "85         ingredient\n",
       "86         ingredient\n",
       "87         ingredient\n",
       "88      dtfingredient\n",
       "89        dtffoodkind\n",
       "90       dtfappetizer\n",
       "91      dtfingredient\n",
       "92         ingredient\n",
       "93                  n\n",
       "94                  n\n",
       "95      dtffoodsaftyb\n",
       "96         ingredient\n",
       "97      dtffoodsaftya\n",
       "98         ingredient\n",
       "99           dtfdrink\n",
       "100          dtfbrand\n",
       "101    dtfxiaolongbao\n",
       "102      dtfappetizer\n",
       "103     dtffoodsaftya\n",
       "104    dtfbranchkinda\n",
       "105           dtfsoup\n",
       "106     dtfingredient\n",
       "107                 n\n",
       "108           dtfsoup\n",
       "Name: NPOS, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NN_POS_PAIR['NPOS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtffoodsaftya\tu'瘦肉精'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>安內</td>\n",
       "      <td>0.501488089561</td>\n",
       "      <td>乳源</td>\n",
       "      <td>0.499897748232</td>\n",
       "      <td>無菌</td>\n",
       "      <td>0.481448233128</td>\n",
       "      <td>和氣生財</td>\n",
       "      <td>0.480949193239</td>\n",
       "      <td>新聞稿</td>\n",
       "      <td>0.480463385582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>黑肚</td>\n",
       "      <td>0.479298591614</td>\n",
       "      <td>管太多</td>\n",
       "      <td>0.475521743298</td>\n",
       "      <td>口水戰</td>\n",
       "      <td>0.471942275763</td>\n",
       "      <td>thinking</td>\n",
       "      <td>0.470590233803</td>\n",
       "      <td>編入</td>\n",
       "      <td>0.469600766897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0               1    2               3    4               5         6  \\\n",
       "0  安內  0.501488089561   乳源  0.499897748232   無菌  0.481448233128      和氣生財   \n",
       "1  黑肚  0.479298591614  管太多  0.475521743298  口水戰  0.471942275763  thinking   \n",
       "\n",
       "                7    8               9  \n",
       "0  0.480949193239  新聞稿  0.480463385582  \n",
       "1  0.470590233803   編入  0.469600766897  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 單一 term 測試 word2vector\n",
    "idx = 5\n",
    "NN_NPOS = NN_POS_PAIR['NPOS'][idx]\n",
    "NN_NTERM= NN_POS_PAIR['NTERM'][idx]\n",
    "print NN_NPOS + '\\t' + repr(NN_NTERM).decode('unicode-escape')\n",
    "w2v_result = model.most_similar(NN_NTERM)\n",
    "pp = pd.DataFrame(np.array(w2v_result))\n",
    "pos_numpy = pd.DataFrame.as_matrix(pp)\n",
    "DATA_STATISTIC_RESHAPE = pd.DataFrame(pos_numpy.reshape(2,-1))\n",
    "DATA_STATISTIC_RESHAPE\n",
    "#exception_case = ['鼎泰豐']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#測試延伸 data frame\n",
    "w2v_np_result = np.array(w2v_result)\n",
    "w2v_np_result = w2v_np_result[:,0] #刪除成績部分,只留下term\n",
    "data = {'NPOS': np.array(['fooe']*10),'NTERM':w2v_np_result}\n",
    "#data = {'NPOS': np.array([NN_POS_PAIR['NPOS'][0]]*10),'NTERM':w2v_np_result}\n",
    "\n",
    "NN_POS_PAIR.append(pd.DataFrame(data),ignore_index=True)\n",
    "#pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 對 dataframe做 reshape (10,2) --> (2,10)\n",
    "pos_numpy = pd.DataFrame.as_matrix(pp)\n",
    "DATA_STATISTIC_RESHAPE = pd.DataFrame(pos_numpy.reshape(2,-1))\n",
    "DATA_STATISTIC_RESHAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN_POS_PAIR['NPOS'][0]\n",
    "id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(NN_POS_PAIR_EXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN_POS_PAIR_EXT = NN_POS_PAIR_EXT.drop_duplicates()\n",
    "NN_POS_PAIR_EXT.sort(['NPOS','NTERM'],ascending=[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN_POS_PAIR_EXT = pd.read_pickle('./NN_POS_PAIR_EXT')\n",
    "NV_POS_PAIR_EXT = pd.read_pickle('./NV_POS_PAIR_EXT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN_POS_PAIR_EXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NN_POS_PAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
